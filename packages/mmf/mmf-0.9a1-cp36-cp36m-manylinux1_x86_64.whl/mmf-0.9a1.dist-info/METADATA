Metadata-Version: 2.1
Name: mmf
Version: 0.9a1
Summary: mmf: a modular framework for vision and language multimodal research.
Home-page: UNKNOWN
Author: Facebook AI Research
Author-email: mmf@fb.com
License: UNKNOWN
Platform: UNKNOWN
Description-Content-Type: text/markdown
Requires-Dist: torch (==1.4.0)
Requires-Dist: torchvision (==0.5.0)
Requires-Dist: tensorboard (==2.1.0)
Requires-Dist: numpy (==1.16.6)
Requires-Dist: tqdm (==4.43.0)
Requires-Dist: demjson (==2.2.4)
Requires-Dist: torchtext (==0.5.0)
Requires-Dist: GitPython (==3.1.0)
Requires-Dist: PyYAML (==5.1.2)
Requires-Dist: pytest (==5.4.1)
Requires-Dist: requests (==2.23.0)
Requires-Dist: fasttext (==0.9.1)
Requires-Dist: nltk (==3.4.1)
Requires-Dist: editdistance (==0.5.3)
Requires-Dist: transformers (==2.3.0)
Requires-Dist: sklearn (==0.0)
Requires-Dist: omegaconf (==2.0.0rc16)
Requires-Dist: lmdb (==0.98)

# MMF

MMF is a modular framework for vision and language multimodal research. Built on top of PyTorch, it features:

- **Multi-Tasking**: Support for multi-tasking which allows training on multiple dataset together.
- **Datasets**: Includes support for various datasets built-in including VQA, VizWiz, TextVQA, VisualDialog and COCO Captioning.
- **Modules**: Provides implementations for many commonly used layers in vision and language domain
- **Distributed**: Support for distributed training based on DataParallel as well as DistributedDataParallel.
- **Unopinionated**: Unopinionated about the dataset and model implementations built on top of it.
- **Customization**: Custom losses, metrics, scheduling, optimizers, tensorboard; suits all your custom needs.

You can use MMF to **_bootstrap_** for your next vision and language multimodal research project.

MMF can also act as **starter codebase** for challenges around vision and
language datasets (TextVQA challenge, VQA challenge)

![MMF Examples](https://i.imgur.com/BP8sYnk.jpg)

## Demo

1. [VQA](https://colab.research.google.com/drive/1Z9fsh10rFtgWe4uy8nvU4mQmqdokdIRR).
2. [Captioning](https://colab.research.google.com/drive/1vzrxDYB0vxtuUy8KCaGxm--nDCJvyBSg).

## Documentation

Learn more about MMF [here](https://learnpythia.readthedocs.io/en/latest/).

## Citation

If you use MMF in your work, please cite:

```
@inproceedings{singh2018pythia,
  title={Pythia-a platform for vision \& language research},
  author={Singh, Amanpreet and Goswami, Vedanuj and Natarajan, Vivek and Jiang, Yu and Chen, Xinlei and Shah, Meet and Rohrbach, Marcus and Batra, Dhruv and Parikh, Devi},
  booktitle={SysML Workshop, NeurIPS},
  volume={2018},
  year={2018}
}
```

## License

MMF is licensed under BSD license available in [LICENSE](LICENSE) file

