# Configuration for training
training:
    # Name of the trainer class used to define the training/evalution loop
    trainer: 'base_trainer'
    # Seed to be used for training. -1 means random seed between 1 and 100000.
    # Either pass fixed through your config or command line arguments
    seed: null
    # Name of the experiment, will be used while saving checkpoints
    # and generating reports
    experiment_name: run
    # Maximum number of iterations the training will run
    max_updates: 22000
    # Maximum epochs in case you don't want to use max_updates
    # Can be mixed with max iterations, so it will stop whichever is
    # completed first. Default: null means epochs won't be used
    max_epochs: null

    # Type of run, train+inference by default means both training and inference
    # (test) stage will be run, if run_type contains 'val',
    # inference will be run on val set also.
    run_type: train_inference

    # Directory for saving checkpoints and other metadata
    save_dir: "./save"

    # After `log_interval` iterations, current iteration's training loss and
    # metrics will be reported. This will also report validation
    # loss and metrics on a single batch from validation set
    # to provide an estimate on validation side
    log_interval: 100
    # Directory for saving logs, default is "logs" inside the save folder
    # If log_dir is specifically passed, logs will be written inside that folder
    log_dir: null
    # Level of logging, only logs which are >= to current level will be logged
    logger_level: info
    # Log format: json, simple
    log_format: simple
    # Whether to log detailed final configuration parameters
    log_detailed_config: false
    # Whether MMF should log or not, Default: False, which means
    # mmf will log by default
    should_not_log: false

    # Tensorboard control, by default tensorboard is disabled
    tensorboard: false
    # Log directory for tensorboard, default points to same as logs
    tensorboard_logdir: null

    # Size of each batch. If distributed or data_parallel
    # is used, this will be divided equally among GPUs
    batch_size: 512
    # Number of workers to be used in dataloaders
    num_workers: 4
    # Some datasets allow fast reading by loading everything in the memory
    # Use this to enable it
    fast_read: false
    # Whether JSON files for evalai evaluation should be generated
    evalai_inference: false
    # Use in multi-tasking, when you want to sample tasks proportional to their sizes
    dataset_size_proportional_sampling: true
    # Whether to pin memory in dataloader
    pin_memory: false

    # After `checkpoint_interval` iterations, MMF will make a snapshot
    # which will involve creating a checkpoint for current training scenarios
    checkpoint_interval: 1000
    # This will evaluate validation metrics on whole validation set after evaluation interval
    evaluation_interval: 1000
    # Whether gradients should be clipped
    clip_gradients: false
    # Mode for clip norm
    clip_norm_mode: all

    # Whether to use early stopping, (Default: false)
    should_early_stop: false
    # Patience for early stopping
    patience: 4000
    # Metric to be monitored for early stopping
    # loss will monitor combined loss from all of the tasks
    # Usually, it will be of the form `dataset_metric`
    # for e.g. vqa2_vqa_accuracy
    monitored_metric: total_loss
    # Whether the monitored metric should be minimized for early stopping
    # or not, for e.g. you would want to minimize loss but maximize accuracy
    metric_minimize: true

    # Should a lr scheduler be used
    lr_scheduler: false
    # Steps for LR scheduler, will be an array of iteration count
    # when lr should be decreased
    lr_steps: []
    # Ratio for each lr step
    lr_ratio: 0.1

    # Should use warmup for lr
    use_warmup: false
    # Warmup factor learning rate warmup
    warmup_factor: 0.2
    # Iteration until which warnup should be done
    warmup_iterations: 1000

    # Local rank of the GPU device
    device: cuda
    local_rank: null

    # Use to load specific modules from checkpoint to your model,
    # this is helpful in finetuning. for e.g. you can specify
    # text_embeddings: text_embedding_pythia
    # for loading `text_embedding` module of your model
    # from `text_embedding_pythia`
    pretrained_mapping: {}
    # If using a pretrained model. Must be used with --resume_file parameter
    # to specify pretrained model checkpoint. Will load only specific layers if
    # pretrained mapping is specified in config
    load_pretrained: false
    # If resume is true, MMF will try to load automatically load
    # last of same parameters from save_dir
    resume: false
    # `resume_file` can be used to load a specific checkpoint from a file
    resume_file: null
    # `resume_best` will load the best checkpoint according to monitored metric instead of
    # the last saved ckpt
    resume_best: false

    # If verbose dump is active, MMF will dump dataset, model specific
    # information which can be useful in debugging
    verbose_dump: false

    # Turn on if you want to ignore unused parameters in case of DDP
    find_unused_parameters: false


# Configuration for models, default configuration files for various models
# included in MMF can be found under configs directory in root folder
model_config: {}

# Configuration for datasets. Separate configuration
# for different datasets included in MMF are included in dataset folder
# which can be mixed and matched to train multiple datasets together
# An example for mixing all vqa datasets is present under vqa folder
dataset_config: {}

# Defines which datasets from the above tasks you want to train on
datasets: []

# Defines which model you want to train on
model: null

# Config file to be optionally passed by the user
config: null

# Configuration for optimizer, examples can be found in models' configs in
# configs folder
optimizer: {}

# Configuration for scheduler, examples can be found in models' configs
scheduler: {}

# Common configurations for MMF
mmf_config:
    # Universal cache directory for mmf
    cache_dir: ./mmf_cache

# Configuration for the distributed setup
distributed:
    # Typically tcp://hostname:port that will be used to establish initial connection
    init_method: null
    # Rank of the current worker
    rank: 0
    # Port number, not required if using init_method,
    port: -1
    # Backend for distributed setup
    backend: nccl
    # Total number of GPUs across all nodes (default: all visible GPUs)
    world_size: ${device_count:}
    # Set if you do not want spawn multiple processes even if
    # multiple GPUs are visible
    no_spawn: false
