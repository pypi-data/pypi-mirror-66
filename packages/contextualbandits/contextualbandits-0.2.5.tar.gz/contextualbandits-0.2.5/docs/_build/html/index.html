

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Contextual Bandits &mdash; Contextual Bandits 0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="Contextual Bandits 0.1 documentation" href="#"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="#" class="icon icon-home"> Contextual Bandits
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Contextual Bandits</a></li>
<li><a class="reference internal" href="#installation">Installation</a></li>
<li><a class="reference internal" href="#getting-started">Getting started</a></li>
<li><a class="reference internal" href="#id1">Online Contextual Bandits</a></li>
<li><a class="reference internal" href="#module-contextualbandits.offpolicy">Off-policy learning</a></li>
<li><a class="reference internal" href="#module-contextualbandits.evaluation">Policy Evaluation</a></li>
<li><a class="reference internal" href="#indices-and-tables">Indices and tables</a></li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">Contextual Bandits</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="#">Docs</a> &raquo;</li>
        
      <li>Contextual Bandits</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="contextual-bandits">
<h1>Contextual Bandits<a class="headerlink" href="#contextual-bandits" title="Permalink to this headline">¶</a></h1>
<p>This is the documentation page for the python package <em>contextualbandits</em>. For
more details, see the project’s home page:</p>
<p><a class="reference external" href="https://www.github.comdavid-cortes/contextualbandits/">https://www.github.comdavid-cortes/contextualbandits/</a></p>
</div>
<div class="section" id="installation">
<h1>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h1>
<p>Package is available on PyPI, can be installed with</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">contextualbandits</span>
</pre></div>
</div>
</div>
<div class="section" id="getting-started">
<h1>Getting started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h1>
<p>You can find user guides with detailed examples in the following links:</p>
<p><a class="reference external" href="http://nbviewer.jupyter.org/github/david-cortes/contextualbandits/blob/master/example/online_contextual_bandits.ipynb">Online Contextual Bandits</a></p>
<p><a class="reference external" href="http://nbviewer.jupyter.org/github/david-cortes/contextualbandits/blob/master/example/offpolicy_learning.ipynb">Off policy Learning in Contextual Bandits</a></p>
<p><a class="reference external" href="http://nbviewer.jupyter.org/github/david-cortes/contextualbandits/blob/master/example/policy_evaluation.ipynb">Policy Evaluation in Contextual Bandits</a></p>
</div>
<div class="section" id="id1">
<h1>Online Contextual Bandits<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<span class="target" id="module-contextualbandits.online"></span><dl class="class">
<dt id="contextualbandits.online.ActiveExplorer">
<em class="property">class </em><code class="descclassname">contextualbandits.online.</code><code class="descname">ActiveExplorer</code><span class="sig-paren">(</span><em>nchoices</em>, <em>C=None</em>, <em>explore_prob=0.15</em>, <em>decay=0.9997</em>, <em>beta_prior='auto'</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.ActiveExplorer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Active Explorer</p>
<p>Logistic Regression which selects a proportion of actions according to an
active learning heuristic based on gradient.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Here, for the predictions that are made according to an active learning heuristic
(these are selected at random, just like in Epsilon-Greedy), the guiding heuristic
is the gradient that the observation, having either label (either weighted by the estimted
probability, or taking the maximum or minimum), would produce on each model that
predicts a class, given the current coefficients for that model.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>nchoices</strong> (<em>int</em>) – Number of arms/labels to choose from.</li>
<li><strong>C</strong> (<em>float</em>) – Inverse of the regularization parameter for Logistic regression.
For more details see sklearn.linear_model.LogisticRegression.</li>
<li><strong>explore_prob</strong> (<em>float (0,1)</em>) – Probability of selecting an action according to active learning criteria.</li>
<li><strong>decay</strong> (<em>float (0,1)</em>) – After each prediction, the probability of selecting an arm according to active
learning criteria is set to p = p*decay</li>
<li><strong>beta_prior</strong> (<em>str ‘auto’, None, or tuple ((a,b), n)</em>) – If not None, when there are less than ‘n’ positive sampless from a class
(actions from that arm that resulted in a reward), it will predict the score
for that class as a random number drawn from a beta distribution with the prior
specified by ‘a’ and ‘b’. If set to auto, will be calculated as:
beta_prior = ((3/nchoices,4), 1)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="contextualbandits.online.ActiveExplorer.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>a</em>, <em>r</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.ActiveExplorer.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the base algorithm (one per class) to partially labeled data with actions chosen by this same policy.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Matrix of covariates for the available data.</li>
<li><strong>a</strong> (<em>array (n_samples), int type</em>) – Arms or actions that were chosen for each observations.</li>
<li><strong>r</strong> (<em>array (n_samples), {0,1}</em>) – Rewards that were observed for the chosen actions. Must be binary rewards 0/1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong> – Copy of this same object</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">obj</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.ActiveExplorer.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>exploit=False</em>, <em>gradient_calc='weighted'</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.ActiveExplorer.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects actions according to this policy for new data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – New observations for which to choose an action according to this policy.</li>
<li><strong>exploit</strong> (<em>bool</em>) – Whether to make a prediction according to the policy, or to just choose the
arm with the highest expected reward according to current models.</li>
<li><strong>gradient_calc</strong> (<em>str, one of ‘weighted’, ‘max’ or ‘min’</em>) – How to calculate the gradient that an observation would have on the loss
function for each classifier, given that it could be either class (positive or negative)
for the classifier that predicts each arm. If weighted, they are weighted by the same
probability estimates from the base algorithm.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>pred</strong> – Actions chosen by the policy.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array (n_samples,)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="contextualbandits.online.AdaptiveGreedy">
<em class="property">class </em><code class="descclassname">contextualbandits.online.</code><code class="descname">AdaptiveGreedy</code><span class="sig-paren">(</span><em>base_algorithm</em>, <em>nchoices</em>, <em>window_size=500</em>, <em>percentile=30</em>, <em>decay=0.9998</em>, <em>decay_type='threshold'</em>, <em>initial_thr='auto'</em>, <em>fixed_thr=False</em>, <em>beta_prior='auto'</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.AdaptiveGreedy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Adaptive Greedy</p>
<p>Takes the action with highest estimated reward, unless that estimation
falls below a certain moving threshold, in which case it takes a random action.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>The threshold for the reward probabilities can be set to a hard-coded number, or
to be calculated dynamically by keeping track of the predictions it makes, and taking
a fixed percentile of that distribution to be the threshold.
In the second case, these are calculated in separate batches rather than in a sliding window.</p>
<p class="last">The original idea was taken from the paper in the references and adapted to the
contextual bandits setting like this.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last">
<li><p class="first"><strong>base_algorithm</strong> (<em>obj</em>) – Base binary classifier for which each sample for each class will be fit.</p>
</li>
<li><p class="first"><strong>nchoices</strong> (<em>int</em>) – Number of arms/labels to choose from.</p>
</li>
<li><p class="first"><strong>window_size</strong> (<em>int</em>) – Number of predictions after which the threshold will be updated to the desired percentile.
Ignored when passing fixed_thr=False</p>
</li>
<li><p class="first"><strong>percentile</strong> (<em>int [0,100]</em>) – Percentile of the predictions sample to set as threshold, below which actions are random.
Ignored in fixed threshold mode.</p>
</li>
<li><p class="first"><strong>decay</strong> (<em>float (0,1)</em>) –</p>
<dl class="docutils">
<dt>After each prediction, either the threshold or the percentile gets adjusted to:</dt>
<dd><p class="first last">val_t+1 = val_t*decay</p>
</dd>
</dl>
<p>Ignored when pasing fixed_thr=True.</p>
</li>
<li><p class="first"><strong>decay_type</strong> (<em>str, either ‘percentile’ or ‘threshold’</em>) – Whether to decay the threshold itself or the percentile of the predictions to take after
each prediction. If set to ‘threshold’ and fixed_thr=False, the threshold will be
recalculated to the same percentile the next time it is updated, but with the latest predictions.
Ignored when passing fixed_thr=True.</p>
</li>
<li><p class="first"><strong>initial_thr</strong> (<em>str ‘autho’ or float (0,1)</em>) – Initial threshold for the prediction below which a random action is taken.
If set to ‘auto’, will be calculated as initial_thr = 1.5/nchoices</p>
</li>
<li><p class="first"><strong>fixed_thr</strong> (<em>bool</em>) – Whether the threshold is to be kept fixed, or updated to a percentile after N predictions.</p>
</li>
<li><p class="first"><strong>beta_prior</strong> (<em>str ‘auto’, None, or tuple ((a,b), n)</em>) – If not None, when there are less than ‘n’ positive sampless from a class
(actions from that arm that resulted in a reward), it will predict the score
for that class as a random number drawn from a beta distribution with the prior
specified by ‘a’ and ‘b’. If set to auto, will be calculated as:
beta_prior = ((3/nchoices,4), 1)</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<p>[1] Mortal multi-armed bandits (2009)</p>
<dl class="method">
<dt id="contextualbandits.online.AdaptiveGreedy.decision_function">
<code class="descname">decision_function</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.AdaptiveGreedy.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the estimated probability for each arm from the classifier that predicts it.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is quite different from the decision_function of the other policies, as it
doesn’t follow the policy in assigning random choices with some probability.
A sigmoid function is applyed to the decision_function of the classifier if it doesn’t
have a predict_proba method.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Data for which to obtain decision function scores for each arm.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>scores</strong> – Scores following this policy for each arm.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">array (n_samples, n_choices)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.AdaptiveGreedy.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>a</em>, <em>r</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.AdaptiveGreedy.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the base algorithm (one per class) to partially labeled data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Matrix of covariates for the available data.</li>
<li><strong>a</strong> (<em>array (n_samples), int type</em>) – Arms or actions that were chosen for each observations.</li>
<li><strong>r</strong> (<em>array (n_samples), {0,1}</em>) – Rewards that were observed for the chosen actions. Must be binary rewards 0/1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong> – Copy of this same object</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">obj</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.AdaptiveGreedy.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>exploit=False</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.AdaptiveGreedy.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects actions according to this policy for new data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – New observations for which to choose an action according to this policy.</li>
<li><strong>exploit</strong> (<em>bool</em>) – Whether to make a prediction according to the policy, or to just choose the
arm with the highest expected reward according to current models.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>pred</strong> – Actions chosen by the policy.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array (n_samples,)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="contextualbandits.online.BayesianTS">
<em class="property">class </em><code class="descclassname">contextualbandits.online.</code><code class="descname">BayesianTS</code><span class="sig-paren">(</span><em>nchoices</em>, <em>method='advi'</em>, <em>beta_prior=((1</em>, <em>1)</em>, <em>3)</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.BayesianTS" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Bayesian Thompson Sampling</p>
<p>Performs Thompson Sampling by sampling a set of Logistic Regression coefficients
from each class, then predicting the class with highest estimate.
.. note:</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">The</span> <span class="n">implementation</span> <span class="n">here</span> <span class="n">uses</span> <span class="n">PyMC3</span><span class="s1">&#39;s GLM formula with default parameters and ADVI.</span>
<span class="n">You</span> <span class="n">might</span> <span class="n">want</span> <span class="n">to</span> <span class="k">try</span> <span class="n">building</span> <span class="n">a</span> <span class="n">different</span> <span class="n">one</span> <span class="n">yourself</span> <span class="kn">from</span> <span class="nn">PyMC3</span> <span class="ow">or</span> <span class="n">Edward</span><span class="o">.</span>
<span class="n">The</span> <span class="n">method</span> <span class="k">as</span> <span class="n">implemented</span> <span class="n">here</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">scalable</span> <span class="n">to</span> <span class="n">high</span><span class="o">-</span><span class="n">dimensional</span> <span class="ow">or</span> <span class="n">big</span> <span class="n">datasets</span><span class="o">.</span>
</pre></div>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>nchoices</strong> (<em>int</em>) – Number of arms/labels to choose from.</li>
<li><strong>method</strong> (<em>str, either ‘advi’ or ‘nuts’</em>) – Method used to sample coefficients (see PyMC3’s documentation for mode details).</li>
<li><strong>beta_prior</strong> (<em>None, or tuple ((a,b), n)</em>) – If not None, when there are less than ‘n’ positive sampless from a class
(actions from that arm that resulted in a reward), it will predict the score
for that class as a random number drawn from a beta distribution with the prior
specified by ‘a’ and ‘b’.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<p>[1] An empirical evaluation of thompson sampling (2011)</p>
<dl class="method">
<dt id="contextualbandits.online.BayesianTS.decision_function">
<code class="descname">decision_function</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.BayesianTS.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the scores for each arm following this policy’s action-choosing criteria.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Data for which to obtain decision function scores for each arm.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>scores</strong> – Scores following this policy for each arm.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">array (n_samples, n_choices)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.BayesianTS.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>a</em>, <em>r</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.BayesianTS.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples coefficients for Logistic Regression models from partially-labeled data, with
actions chosen by this same policy.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Matrix of covariates for the available data.</li>
<li><strong>a</strong> (<em>array (n_samples), int type</em>) – Arms or actions that were chosen for each observations.</li>
<li><strong>r</strong> (<em>array (n_samples), {0,1}</em>) – Rewards that were observed for the chosen actions. Must be binary rewards 0/1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong> – Copy of this same object</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">obj</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.BayesianTS.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>exploit=False</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.BayesianTS.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects actions according to this policy for new data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – New observations for which to choose an action according to this policy.</li>
<li><strong>exploit</strong> (<em>bool</em>) – Whether to make a prediction according to the policy, or to just choose the
arm with the highest expected reward according to current models.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>pred</strong> – Actions chosen by the policy.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array (n_samples,)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="contextualbandits.online.BayesianUCB">
<em class="property">class </em><code class="descclassname">contextualbandits.online.</code><code class="descname">BayesianUCB</code><span class="sig-paren">(</span><em>nchoices</em>, <em>percentile=80</em>, <em>method='advi'</em>, <em>nsamples=None</em>, <em>beta_prior=((3</em>, <em>1)</em>, <em>3)</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.BayesianUCB" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Bayesian Upper-Confidence Bound</p>
<p>Gets an upper-confidence bound by Bayesian Logistic Regression estimates.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The implementation here uses PyMC3’s GLM formula with default parameters and ADVI.
You might want to try building a different one yourself from PyMC3 or Edward.
The method as implemented here is not scalable to high-dimensional or big datasets.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>nchoices</strong> (<em>int</em>) – Number of arms/labels to choose from.</li>
<li><strong>percentile</strong> (<em>int [0,100]</em>) – Percentile of the predictions sample to take</li>
<li><strong>method</strong> (<em>str, either ‘advi’ or ‘nuts’</em>) – Method used to sample coefficients (see PyMC3’s documentation for mode details).</li>
<li><strong>beta_prior</strong> (<em>None, or tuple ((a,b), n)</em>) – If not None, when there are less than ‘n’ positive sampless from a class
(actions from that arm that resulted in a reward), it will predict the score
for that class as a random number drawn from a beta distribution with the prior
specified by ‘a’ and ‘b’.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="contextualbandits.online.BayesianUCB.decision_function">
<code class="descname">decision_function</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.BayesianUCB.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the scores for each arm following this policy’s action-choosing criteria.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Data for which to obtain decision function scores for each arm.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>scores</strong> – Scores following this policy for each arm.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">array (n_samples, n_choices)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.BayesianUCB.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>a</em>, <em>r</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.BayesianUCB.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples Logistic Regression coefficients for partially labeled data with actions chosen by this policy.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Matrix of covariates for the available data.</li>
<li><strong>a</strong> (<em>array (n_samples), int type</em>) – Arms or actions that were chosen for each observations.</li>
<li><strong>r</strong> (<em>array (n_samples), {0,1}</em>) – Rewards that were observed for the chosen actions. Must be binary rewards 0/1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong> – Copy of this same object</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">obj</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.BayesianUCB.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>exploit=False</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.BayesianUCB.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects actions according to this policy for new data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – New observations for which to choose an action according to this policy.</li>
<li><strong>exploit</strong> (<em>bool</em>) – Whether to make a prediction according to the policy, or to just choose the
arm with the highest expected reward according to current models.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>pred</strong> – Actions chosen by the policy.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array (n_samples,)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="contextualbandits.online.BootstrappedTS">
<em class="property">class </em><code class="descclassname">contextualbandits.online.</code><code class="descname">BootstrappedTS</code><span class="sig-paren">(</span><em>base_algorithm</em>, <em>nchoices</em>, <em>nsamples=10</em>, <em>beta_prior='auto'</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.BootstrappedTS" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Bootstrapped Thompson Sampling</p>
<p>Performs Thompson Sampling by fitting several models per class on bootstrapped samples,
then makes predictions by taking one of them at random for each class.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>base_algorithm</strong> (<em>obj</em>) – Base binary classifier for which each sample for each class will be fit.</li>
<li><strong>nchoices</strong> (<em>int</em>) – Number of arms/labels to choose from.</li>
<li><strong>nsamples</strong> (<em>int</em>) – Number of bootstrapped samples per class to take.</li>
<li><strong>beta_prior</strong> (<em>str ‘auto’, None, or tuple ((a,b), n)</em>) – If not None, when there are less than ‘n’ positive sampless from a class
(actions from that arm that resulted in a reward), it will predict the score
for that class as a random number drawn from a beta distribution with the prior
specified by ‘a’ and ‘b’. If set to auto, will be calculated as:
beta_prior = ((3/nchoices,4), 1)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<p>[1] An empirical evaluation of thompson sampling (2011)</p>
<dl class="method">
<dt id="contextualbandits.online.BootstrappedTS.decision_function">
<code class="descname">decision_function</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.BootstrappedTS.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the scores for each arm following this policy’s action-choosing criteria.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Data for which to obtain decision function scores for each arm.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>scores</strong> – Scores following this policy for each arm.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">array (n_samples, n_choices)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.BootstrappedTS.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>a</em>, <em>r</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.BootstrappedTS.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the base algorithm (one per sample per class) to partially labeled data,
with the actions having been determined by this same policy.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Matrix of covariates for the available data.</li>
<li><strong>a</strong> (<em>array (n_samples), int type</em>) – Arms or actions that were chosen for each observations.</li>
<li><strong>r</strong> (<em>array (n_samples), {0,1}</em>) – Rewards that were observed for the chosen actions. Must be binary rewards 0/1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong> – Copy of this same object</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">obj</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.BootstrappedTS.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>exploit=False</em>, <em>output_score=False</em>, <em>apply_sigmoid_scores=True</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.BootstrappedTS.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects actions according to this policy for new data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – New observations for which to choose an action according to this policy.</li>
<li><strong>exploit</strong> (<em>bool</em>) – Whether to make a prediction according to the policy, or to just choose the
arm with the highest expected reward according to current models.</li>
<li><strong>output_score</strong> (<em>bool</em>) – Whether to output the score that this method predicted, in case it is desired to use
it with this pakckage’s offpolicy and evaluation modules.</li>
<li><strong>apply_sigmoid_scores</strong> (<em>bool</em>) – If passing output_score=True, whether to apply a sigmoid function to the scores
from the decision function of the classifier that predicts each class.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>pred</strong> – Actions chosen by the policy. If passing output_score=True, it will be an array
with the first column indicating the action and the second one indicating the score
that the classifier gave to that class.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array (n_samples,) or (n_samples, 2)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="contextualbandits.online.BootstrappedUCB">
<em class="property">class </em><code class="descclassname">contextualbandits.online.</code><code class="descname">BootstrappedUCB</code><span class="sig-paren">(</span><em>base_algorithm</em>, <em>nchoices</em>, <em>nsamples=10</em>, <em>percentile=80</em>, <em>beta_prior='auto'</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.BootstrappedUCB" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<dl class="method">
<dt id="contextualbandits.online.BootstrappedUCB.decision_function">
<code class="descname">decision_function</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.BootstrappedUCB.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the scores for each arm following this policy’s action-choosing criteria.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Data for which to obtain decision function scores for each arm.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>scores</strong> – Scores following this policy for each arm.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">array (n_samples, n_choices)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.BootstrappedUCB.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>a</em>, <em>r</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.BootstrappedUCB.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the base algorithm (one per sample per class) to partially labeled data,
with the actions having been determined by this same policy.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Matrix of covariates for the available data.</li>
<li><strong>a</strong> (<em>array (n_samples), int type</em>) – Arms or actions that were chosen for each observations.</li>
<li><strong>r</strong> (<em>array (n_samples), {0,1}</em>) – Rewards that were observed for the chosen actions. Must be binary rewards 0/1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong> – Copy of this same object</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">obj</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.BootstrappedUCB.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>exploit=False</em>, <em>output_score=False</em>, <em>apply_sigmoid_scores=True</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.BootstrappedUCB.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects actions according to this policy for new data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – New observations for which to choose an action according to this policy.</li>
<li><strong>exploit</strong> (<em>bool</em>) – Whether to make a prediction according to the policy, or to just choose the
arm with the highest expected reward according to current models.</li>
<li><strong>output_score</strong> (<em>bool</em>) – Whether to output the score that this method predicted, in case it is desired to use
it with this pakckage’s offpolicy and evaluation modules.</li>
<li><strong>apply_sigmoid_scores</strong> (<em>bool</em>) – If passing output_score=True, whether to apply a sigmoid function to the scores
from the decision function of the classifier that predicts each class.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>pred</strong> – Actions chosen by the policy. If passing output_score=True, it will be an array
with the first column indicating the action and the second one indicating the score
that the classifier gave to that class.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array (n_samples,) or (n_samples, 2)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="contextualbandits.online.EpsilonGreedy">
<em class="property">class </em><code class="descclassname">contextualbandits.online.</code><code class="descname">EpsilonGreedy</code><span class="sig-paren">(</span><em>base_algorithm</em>, <em>nchoices</em>, <em>explore_prob=0.2</em>, <em>decay=0.9999</em>, <em>beta_prior='auto'</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.EpsilonGreedy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Epsilon Greedy</p>
<p>Takes a random action with probability p, or the action with highest
estimated reward with probability 1-p.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>base_algorithm</strong> (<em>obj</em>) – Base binary classifier for which each sample for each class will be fit.</li>
<li><strong>nchoices</strong> (<em>int</em>) – Number of arms/labels to choose from.</li>
<li><strong>explore_prob</strong> (<em>float (0,1)</em>) – Probability of taking a random action at each round.</li>
<li><strong>decay</strong> (<em>float (0,1)</em>) – After each prediction, the explore probability reduces to
p = p*decay</li>
<li><strong>beta_prior</strong> (<em>str ‘auto’, None, or tuple ((a,b), n)</em>) – If not None, when there are less than ‘n’ positive sampless from a class
(actions from that arm that resulted in a reward), it will predict the score
for that class as a random number drawn from a beta distribution with the prior
specified by ‘a’ and ‘b’. If set to auto, will be calculated as:
beta_prior = ((3/nchoices,4), 1)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<p>[1] The k-armed dueling bandits problem (2010)</p>
<dl class="method">
<dt id="contextualbandits.online.EpsilonGreedy.decision_function">
<code class="descname">decision_function</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.EpsilonGreedy.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the decision function for each arm from the classifier that predicts it.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is quite different from the decision_function of the other policies, as it
doesn’t follow the policy in assigning random choices with some probability.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Data for which to obtain decision function scores for each arm.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>scores</strong> – Scores following this policy for each arm.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">array (n_samples, n_choices)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.EpsilonGreedy.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>a</em>, <em>r</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.EpsilonGreedy.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the base algorithm (one per class) to partially labeled data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Matrix of covariates for the available data.</li>
<li><strong>a</strong> (<em>array (n_samples), int type</em>) – Arms or actions that were chosen for each observations.</li>
<li><strong>r</strong> (<em>array (n_samples), {0,1}</em>) – Rewards that were observed for the chosen actions. Must be binary rewards 0/1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong> – Copy of this same object</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">obj</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.EpsilonGreedy.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>exploit=False</em>, <em>output_score=False</em>, <em>apply_sigmoid_scores=True</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.EpsilonGreedy.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects actions according to this policy for new data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – New observations for which to choose an action according to this policy.</li>
<li><strong>exploit</strong> (<em>bool</em>) – Whether to make a prediction according to the policy, or to just choose the
arm with the highest expected reward according to current models.</li>
<li><strong>output_score</strong> (<em>bool</em>) – Whether to output the score that this method predicted, in case it is desired to use
it with this pakckage’s offpolicy and evaluation modules.</li>
<li><strong>apply_sigmoid_scores</strong> (<em>bool</em>) – If passing output_score=True, whether to apply a sigmoid function to the scores
from the decision function of the classifier that predicts each class.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>pred</strong> – Actions chosen by the policy. If passing output_score=True, it will be an array
with the first column indicating the action and the second one indicating the score
that the classifier gave to that class.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array (n_samples,) or (n_samples, 2)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="contextualbandits.online.ExploreFirst">
<em class="property">class </em><code class="descclassname">contextualbandits.online.</code><code class="descname">ExploreFirst</code><span class="sig-paren">(</span><em>base_algorithm</em>, <em>nchoices</em>, <em>explore_rounds=2500</em>, <em>beta_prior='auto'</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.ExploreFirst" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Explore First, a.k.a. Explore-Then-Exploit</p>
<p>Selects random actions for the first N predictions, after which it selects the
best arm only according to its estimates.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>base_algorithm</strong> (<em>obj</em>) – Base binary classifier for which each sample for each class will be fit.</li>
<li><strong>nchoices</strong> (<em>int</em>) – Number of arms/labels to choose from.</li>
<li><strong>explore_rounds</strong> (<em>int</em>) – Number of rounds to wait before exploitation mode.
Will switch after making N predictions.</li>
<li><strong>beta_prior</strong> (<em>str ‘auto’, None, or tuple ((a,b), n)</em>) – If not None, when there are less than ‘n’ positive sampless from a class
(actions from that arm that resulted in a reward), it will predict the score
for that class as a random number drawn from a beta distribution with the prior
specified by ‘a’ and ‘b’. If set to auto, will be calculated as:
beta_prior = ((3/nchoices,4), 1)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<p>[1] The k-armed dueling bandits problem (2012)</p>
<dl class="method">
<dt id="contextualbandits.online.ExploreFirst.decision_function">
<code class="descname">decision_function</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.ExploreFirst.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the decision function for each arm from the classifier that predicts it.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This is quite different from the decision_function of the other policies, as it
doesn’t follow the policy in assigning random choices at the beginning with equal
probability all.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Data for which to obtain decision function scores for each arm.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>scores</strong> – Scores following this policy for each arm.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">array (n_samples, n_choices)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.ExploreFirst.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>a</em>, <em>r</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.ExploreFirst.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the base algorithm (one per class) to partially labeled data with actions chosen by this same policy.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Matrix of covariates for the available data.</li>
<li><strong>a</strong> (<em>array (n_samples), int type</em>) – Arms or actions that were chosen for each observations.</li>
<li><strong>r</strong> (<em>array (n_samples), {0,1}</em>) – Rewards that were observed for the chosen actions. Must be binary rewards 0/1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong> – Copy of this same object</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">obj</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.ExploreFirst.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>exploit=False</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.ExploreFirst.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects actions according to this policy for new data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – New observations for which to choose an action according to this policy.</li>
<li><strong>exploit</strong> (<em>bool</em>) – Whether to make a prediction according to the policy, or to just choose the
arm with the highest expected reward according to current models.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>pred</strong> – Actions chosen by the policy.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array (n_samples,)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="contextualbandits.online.LinUCB">
<em class="property">class </em><code class="descclassname">contextualbandits.online.</code><code class="descname">LinUCB</code><span class="sig-paren">(</span><em>nchoices</em>, <em>alpha=1.0</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.LinUCB" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The formula described in the paper where this algorithm first appeared had dimensions
that didn’t match to an array of predictions. I assumed that the (n x n) matrix that
results inside a square root was to be summed by rows.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>nchoices</strong> (<em>int</em>) – Number of arms/labels to choose from.</li>
<li><strong>alpha</strong> (<em>float</em>) – Parameter to control the upper-confidence bound (more is higher).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<p>[1] A contextual-bandit approach to personalized news article recommendation (2010)</p>
<dl class="method">
<dt id="contextualbandits.online.LinUCB.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>a</em>, <em>r</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.LinUCB.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>”
Fits one linear model for the first time to partially labeled data.
Overwrites previously fitted coefficients if there were any.
(See partial_fit for adding more data in batches)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Matrix of covariates for the available data.</li>
<li><strong>a</strong> (<em>array (n_samples), int type</em>) – Arms or actions that were chosen for each observations.</li>
<li><strong>r</strong> (<em>array (n_samples), {0,1}</em>) – Rewards that were observed for the chosen actions. Must be binary rewards 0/1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong> – Copy of this same object</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">obj</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.LinUCB.partial_fit">
<code class="descname">partial_fit</code><span class="sig-paren">(</span><em>X</em>, <em>a</em>, <em>r</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.LinUCB.partial_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>”
Updates each linear model with a new batch of data with actions chosen by this same policy.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Matrix of covariates for the available data.</li>
<li><strong>a</strong> (<em>array (n_samples), int type</em>) – Arms or actions that were chosen for each observations.</li>
<li><strong>r</strong> (<em>array (n_samples), {0,1}</em>) – Rewards that were observed for the chosen actions. Must be binary rewards 0/1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong> – Copy of this same object</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">obj</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.LinUCB.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>exploit=False</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.LinUCB.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects actions according to this policy for new data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – New observations for which to choose an action according to this policy.</li>
<li><strong>exploit</strong> (<em>bool</em>) – Whether to make a prediction according to the policy, or to just choose the
arm with the highest expected reward according to current models.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>pred</strong> – Actions chosen by the policy.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array (n_samples,)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="contextualbandits.online.SeparateClassifiers">
<em class="property">class </em><code class="descclassname">contextualbandits.online.</code><code class="descname">SeparateClassifiers</code><span class="sig-paren">(</span><em>base_algorithm</em>, <em>nchoices</em>, <em>beta_prior=None</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.SeparateClassifiers" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Separate Clasifiers per arm</p>
<p>Fits one classifier per arm using only the data on which that arm was chosen.
Predicts as One-Vs-Rest.</p>
<dl class="docutils">
<dt>base_algorithm <span class="classifier-delimiter">:</span> <span class="classifier">obj</span></dt>
<dd>Base binary classifier for which each sample for each class will be fit.</dd>
<dt>nchoices <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
<dd>Number of arms/labels to choose from.</dd>
<dt>beta_prior <span class="classifier-delimiter">:</span> <span class="classifier">str ‘auto’, None, or tuple ((a,b), n)</span></dt>
<dd>If not None, when there are less than ‘n’ positive sampless from a class
(actions from that arm that resulted in a reward), it will predict the score
for that class as a random number drawn from a beta distribution with the prior
specified by ‘a’ and ‘b’. If set to auto, will be calculated as:
beta_prior = ((3/nchoices,4), 1)</dd>
</dl>
<dl class="method">
<dt id="contextualbandits.online.SeparateClassifiers.decision_function">
<code class="descname">decision_function</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.SeparateClassifiers.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the scores for each arm following this policy’s action-choosing criteria.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Data for which to obtain decision function scores for each arm.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>scores</strong> – Scores following this policy for each arm.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">array (n_samples, n_choices)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.SeparateClassifiers.decision_function_std">
<code class="descname">decision_function_std</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.SeparateClassifiers.decision_function_std" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the predicted probabilities from each arm from the classifier that predicts it,
standardized to sum up to 1.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Classifiers are all fit on different data, so the probabilities will not add up to 1.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Data for which to obtain decision function scores for each arm.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>scores</strong> – Scores following this policy for each arm.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">array (n_samples, n_choices)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.SeparateClassifiers.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>a</em>, <em>r</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.SeparateClassifiers.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the base algorithm (one per class) to partially labeled data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Matrix of covariates for the available data.</li>
<li><strong>a</strong> (<em>array (n_samples), int type</em>) – Arms or actions that were chosen for each observations.</li>
<li><strong>r</strong> (<em>array (n_samples), {0,1}</em>) – Rewards that were observed for the chosen actions. Must be binary rewards 0/1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong> – Copy of this same object</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">obj</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.SeparateClassifiers.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>exploit=False</em>, <em>output_score=False</em>, <em>apply_sigmoid_scores=True</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.SeparateClassifiers.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects actions according to this policy for new data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – New observations for which to choose an action according to this policy.</li>
<li><strong>exploit</strong> (<em>bool</em>) – Whether to make a prediction according to the policy, or to just choose the
arm with the highest expected reward according to current models.</li>
<li><strong>output_score</strong> (<em>bool</em>) – Whether to output the score that this method predicted, in case it is desired to use
it with this pakckage’s offpolicy and evaluation modules.</li>
<li><strong>apply_sigmoid_scores</strong> (<em>bool</em>) – If passing output_score=True, whether to apply a sigmoid function to the scores
from the decision function of the classifier that predicts each class.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>pred</strong> – Actions chosen by the policy. If passing output_score=True, it will be an array
with the first column indicating the action and the second one indicating the score
that the classifier gave to that class.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array (n_samples,) or (n_samples, 2)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.SeparateClassifiers.predict_proba_separate">
<code class="descname">predict_proba_separate</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.SeparateClassifiers.predict_proba_separate" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the predicted probabilities from each arm from the classifier that predicts it.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Classifiers are all fit on different data, so the probabilities will not add up to 1.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Data for which to obtain decision function scores for each arm.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>scores</strong> – Scores following this policy for each arm.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">array (n_samples, n_choices)</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="contextualbandits.online.SoftmaxExplorer">
<em class="property">class </em><code class="descclassname">contextualbandits.online.</code><code class="descname">SoftmaxExplorer</code><span class="sig-paren">(</span><em>base_algorithm</em>, <em>nchoices</em>, <em>beta_prior='auto'</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.SoftmaxExplorer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Soft-Max Explorer</p>
<p>Selects an action according to probabilites determined by a softmax transformation
on the scores from the decision function that predicts each class.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If the base algorithm has ‘predict_proba’, but no ‘decision_function’, it will
calculate the ‘probabilities’ with a simple scaling by sum rather than by a softmax.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>base_algorithm</strong> (<em>obj</em>) – Base binary classifier for which each sample for each class will be fit.</li>
<li><strong>nchoices</strong> (<em>int</em>) – Number of arms/labels to choose from.</li>
<li><strong>beta_prior</strong> (<em>str ‘auto’, None, or tuple ((a,b), n)</em>) – If not None, when there are less than ‘n’ positive sampless from a class
(actions from that arm that resulted in a reward), it will predict the score
for that class as a random number drawn from a beta distribution with the prior
specified by ‘a’ and ‘b’. If set to auto, will be calculated as:
beta_prior = ((3/nchoices,4), 1)</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="contextualbandits.online.SoftmaxExplorer.decision_function">
<code class="descname">decision_function</code><span class="sig-paren">(</span><em>X</em>, <em>output_score=False</em>, <em>apply_sigmoid_score=True</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.SoftmaxExplorer.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Get the scores for each arm following this policy’s action-choosing criteria.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Data for which to obtain decision function scores for each arm.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>scores</strong> – Scores following this policy for each arm.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">array (n_samples, n_choices)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.SoftmaxExplorer.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>a</em>, <em>r</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.SoftmaxExplorer.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the base algorithm (one per class) to partially labeled data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Matrix of covariates for the available data.</li>
<li><strong>a</strong> (<em>array (n_samples), int type</em>) – Arms or actions that were chosen for each observations.</li>
<li><strong>r</strong> (<em>array (n_samples), {0,1}</em>) – Rewards that were observed for the chosen actions. Must be binary rewards 0/1.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>self</strong> – Copy of this same object</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">obj</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.online.SoftmaxExplorer.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em>, <em>exploit=False</em>, <em>output_score=False</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.online.SoftmaxExplorer.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Selects actions according to this policy for new data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – New observations for which to choose an action according to this policy.</li>
<li><strong>exploit</strong> (<em>bool</em>) – Whether to make a prediction according to the policy, or to just choose the
arm with the highest expected reward according to current models.</li>
<li><strong>output_score</strong> (<em>bool</em>) – Whether to output the score that this method predicted, in case it is desired to use
it with this pakckage’s offpolicy and evaluation modules.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>pred</strong> – Actions chosen by the policy. If passing output_score=True, it will be an array
with the first column indicating the action and the second one indicating the score
that the classifier gave to that class.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array (n_samples,) or (n_samples, 2)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-contextualbandits.offpolicy">
<span id="off-policy-learning"></span><h1>Off-policy learning<a class="headerlink" href="#module-contextualbandits.offpolicy" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="contextualbandits.offpolicy.DoublyRobustEstimator">
<em class="property">class </em><code class="descclassname">contextualbandits.offpolicy.</code><code class="descname">DoublyRobustEstimator</code><span class="sig-paren">(</span><em>base_algorithm</em>, <em>reward_estimator</em>, <em>nchoices</em>, <em>method='rovr'</em>, <em>handle_invalid=True</em>, <em>c=None</em>, <em>pmin=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.offpolicy.DoublyRobustEstimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Doubly-Robust Estimator</p>
<p>Estimates the expected reward for each arm, applies a correction for the actions that
were chosen, and converts the problem to const-sensitive classification, on which the
base algorithm is then fit.</p>
<p>This technique converts the problem into a cost-sensitive classification problem
by calculating a matrix of expected rewards and turning it into costs. The base
algorithm is then fit to this data, using either the Weighted All-Pairs approach,
which requires a binary classifier with sample weights as base algorithm, or the
Regression One-Vs-Rest approach, which requires a regressor as base algorithm.</p>
<p>In the Weighted All-Pairs approach, this technique will fail if there are actions that
were never taken by the exploration policy, as it cannot construct a model for them.</p>
<p>The expected rewards are estimated with the imputer algorithm passed here, which should
output a number in the range [0,1].</p>
<p>This technique is meant for the case of contiunous rewards in the [0,1] interval,
but here it is used for the case of discrete rewards {0,1}, under which it performs
poorly. It is not recommended to use, but provided for comparison purposes.</p>
<p>This method requires to form reward estimates of all arms for each observation. In order to do so,
you can either provide estimates as an array (see Parameters), or pass a model.</p>
<p>One method to obtain reward estimates is to fit a model to the data and use its predictions as
reward estimates. You can do so by passing an object of class
<cite>contextualbandits.online.SeparateClassifiers</cite> which should be already fitted, or by passing a
classifier with a ‘predict_proba’ method, which will be put into a ‘SeparateClassifiers’</p>
<blockquote>
<div>object and fit to the same data passed to this function to obtain reward estimates.</div></blockquote>
<p>The estimates can make invalid predictions if there are some arms for which every time
they were chosen they resulted in a reward, or never resulted in a reward. In such cases,
this function includes the option to impute the “predictions” for them (which would otherwise
always be exactly zero or one regardless of the context) by replacing them with random
numbers ~Beta(3,1) or ~Beta(1,3) for the cases of always good and always bad.</p>
<p>This is just a wild idea though, and doesn’t guarantee reasonable results in such siutation.</p>
<p>Note that, if you are using the ‘SeparateClassifiers’ class from the online module in this
same package, it comes with a method ‘predict_proba_separate’ that can be used to get reward
estimates. It still can suffer from the same problem of always-one and always-zero predictions though.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last">
<li><p class="first"><strong>base_algorithm</strong> (<em>obj</em>) – Base algorithm to be used for cost-sensitive classification.</p>
</li>
<li><p class="first"><strong>reward_estimator</strong> (<em>obj or array (n_samples, n_choices)</em>) –</p>
<dl class="docutils">
<dt>One of the following:</dt>
<dd><ul class="first last simple">
<li>An array with the first column corresponding to the reward estimates for the action chosen
by the new policy, and the second column corresponding to the reward estimates for the
action chosen in the data (see Note for details).</li>
<li>An already-fit object of class ‘contextualbandits.online.SeparateClassifiers’, which will
be used to make predictions on the actions chosen and the actions that the new
policy would choose.</li>
<li>A classifier with a ‘predict_proba’ method, which will be fit to the same test data
passed here in order to obtain reward estimates (see Note 2 for details).</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first"><strong>nchoices</strong> (<em>int</em>) – Number of arms/labels to choose from.
Only used when passing a classifier object to ‘reward_estimator’.</p>
</li>
<li><p class="first"><strong>method</strong> (<em>str, either ‘rovr’ or ‘wap’</em>) – Whether to use Regression One-Vs-Rest or Weighted All-Pairs (see Note 1)</p>
</li>
<li><p class="first"><strong>handle_invalid</strong> (<em>bool</em>) – Whether to replace 0/1 estimated rewards with randomly-generated numbers (see Note 2)</p>
</li>
<li><p class="first"><strong>c</strong> (<em>None or float</em>) – Constant by which to multiply all scores from the exploration policy.</p>
</li>
<li><p class="first"><strong>pmin</strong> (<em>None or float</em>) – Scores (from the exploration policy) will be converted to the minimum between
pmin and the original estimate.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<p>[1] Doubly robust policy evaluation and learning (2011)</p>
<p>[2] Doubly robust policy evaluation and optimization (2014)</p>
<dl class="method">
<dt id="contextualbandits.offpolicy.DoublyRobustEstimator.decision_function">
<code class="descname">decision_function</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.offpolicy.DoublyRobustEstimator.decision_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Get score distribution for the arm’s rewards</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">For details on how this is calculated, see the documentation of the
RegressionOneVsRest and WeightedAllPairs classes in the costsensitive package.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array (n_samples, n_features)</em>) – New observations for which to evaluate actions.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>pred</strong> – Score assigned to each arm for each observation (see Note).</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">array (n_samples, n_choices)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.offpolicy.DoublyRobustEstimator.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>a</em>, <em>r</em>, <em>p</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.offpolicy.DoublyRobustEstimator.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the Doubly-Robust estimator to partially-labeled data collected from a different policy.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Matrix of covariates for the available data.</li>
<li><strong>a</strong> (<em>array (n_samples), int type</em>) – Arms or actions that were chosen for each observations.</li>
<li><strong>r</strong> (<em>array (n_samples), {0,1}</em>) – Rewards that were observed for the chosen actions. Must be binary rewards 0/1.</li>
<li><strong>p</strong> (<em>array (n_samples)</em>) – Reward estimates for the actions that were chosen by the policy.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.offpolicy.DoublyRobustEstimator.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.offpolicy.DoublyRobustEstimator.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict best arm for new data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array (n_samples, n_features)</em>) – New observations for which to choose an action.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>pred</strong> – Actions chosen by this technique.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">array (n_samples,)</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="contextualbandits.offpolicy.OffsetTree">
<em class="property">class </em><code class="descclassname">contextualbandits.offpolicy.</code><code class="descname">OffsetTree</code><span class="sig-paren">(</span><em>base_algorithm</em>, <em>nchoices</em>, <em>c=None</em>, <em>pmin=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.offpolicy.OffsetTree" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal"><span class="pre">object</span></code></p>
<p>Offset Tree</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>base_algorithm</strong> (<em>obj</em>) – Binary classifier to be used for each classification sub-problem in the tree.</li>
<li><strong>nchoices</strong> (<em>int</em>) – Number of arms/labels to choose from.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<p>[1] The offset tree for learning with partial labels (2009)</p>
<dl class="method">
<dt id="contextualbandits.offpolicy.OffsetTree.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>X</em>, <em>a</em>, <em>r</em>, <em>p</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.offpolicy.OffsetTree.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fits the Offset Tree estimator to partially-labeled data collected from a different policy.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Matrix of covariates for the available data.</li>
<li><strong>a</strong> (<em>array (n_samples), int type</em>) – Arms or actions that were chosen for each observations.</li>
<li><strong>r</strong> (<em>array (n_samples), {0,1}</em>) – Rewards that were observed for the chosen actions. Must be binary rewards 0/1.</li>
<li><strong>p</strong> (<em>array (n_samples)</em>) – Reward estimates for the actions that were chosen by the policy.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="contextualbandits.offpolicy.OffsetTree.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>X</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.offpolicy.OffsetTree.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict best arm for new data.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">While in theory, making predictions from this algorithm should be faster than from others,
the implementation here uses a Python loop for each observation, which is slow compared to
NumPy array lookups, so the predictions will be slower to calculate than those from other algorithms.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>X</strong> (<em>array (n_samples, n_features)</em>) – New observations for which to choose an action.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><strong>pred</strong> – Actions chosen by this technique.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">array (n_samples,)</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-contextualbandits.evaluation">
<span id="policy-evaluation"></span><h1>Policy Evaluation<a class="headerlink" href="#module-contextualbandits.evaluation" title="Permalink to this headline">¶</a></h1>
<dl class="function">
<dt id="contextualbandits.evaluation.evaluateDoublyRobust">
<code class="descclassname">contextualbandits.evaluation.</code><code class="descname">evaluateDoublyRobust</code><span class="sig-paren">(</span><em>pred</em>, <em>X</em>, <em>a</em>, <em>r</em>, <em>p</em>, <em>reward_estimator</em>, <em>nchoices=None</em>, <em>handle_invalid=True</em>, <em>c=None</em>, <em>pmin=1e-05</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.evaluation.evaluateDoublyRobust" title="Permalink to this definition">¶</a></dt>
<dd><p>Doubly-Robust Policy Evaluation</p>
<p>Evaluates rewards of arm choices of a policy from data collected by another policy.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>This method requires to form reward estimates of the arms that were chosen and of the arms
that the policy to be evaluated would choose. In order to do so, you can either provide
estimates as an array (see Parameters), or pass a model.</p>
<p>One method to obtain reward estimates is to fit a model to both the training and test data
and use its predictions as reward estimates. You can do so by passing an object of class
<cite>contextualbandits.online.SeparateClassifiers</cite> which should be already fitted.</p>
<p>Another method is to fit a model to the test data, in which case you can pass a classifier
with a ‘predict_proba’ method here, which will be fit to the same test data passed to this
function to obtain reward estimates.</p>
<p>The last two options can suffer from invalid predictions if there are some arms for which every time
they were chosen they resulted in a reward, or never resulted in a reward. In such cases,
this function includes the option to impute the “predictions” for them (which would otherwise
always be exactly zero or one regardless of the context) by replacing them with random
numbers ~Beta(3,1) or ~Beta(1,3) for the cases of always good and always bad.</p>
<p>This is just a wild idea though, and doesn’t guarantee reasonable results in such siutation.</p>
<p class="last">Note that, if you are using the ‘SeparateClassifiers’ class from the online module in this
same package, it comes with a method ‘predict_proba_separate’ that can be used to get reward
estimates. It still can suffer from the same problem of always-one and always-zero predictions though.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last">
<li><p class="first"><strong>pred</strong> (<em>array (n_samples,)</em>) – Arms that would be chosen by the policy to evaluate.</p>
</li>
<li><p class="first"><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Matrix of covariates for the available data.</p>
</li>
<li><p class="first"><strong>a</strong> (<em>array (n_samples), int type</em>) – Arms or actions that were chosen for each observation.</p>
</li>
<li><p class="first"><strong>r</strong> (<em>array (n_samples), {0,1}</em>) – Rewards that were observed for the chosen actions. Must be binary rewards 0/1.</p>
</li>
<li><p class="first"><strong>p</strong> (<em>array (n_samples)</em>) – Scores or reward estimates from the policy that generated the data for the actions
that were chosen by it.</p>
</li>
<li><p class="first"><strong>reward_estimator</strong> (<em>obj or array (n_samples, 2)</em>) –</p>
<dl class="docutils">
<dt>One of the following:</dt>
<dd><ul class="first last simple">
<li>An array with the first column corresponding to the reward estimates for the action chosen
by the new policy, and the second column corresponding to the reward estimates for the
action chosen in the data (see Note for details).</li>
<li>An already-fit object of class ‘contextualbandits.online.SeparateClassifiers’, which will
be used to make predictions on the actions chosen and the actions that the new
policy would choose.</li>
<li>A classifier with a ‘predict_proba’ method, which will be fit to the same test data
passed here in order to obtain reward estimates (see Note for details).</li>
</ul>
</dd>
</dl>
</li>
<li><p class="first"><strong>nchoices</strong> (<em>int</em>) – Number of arms/labels to choose from.
Only used when passing a classifier object to ‘reward_estimator’.</p>
</li>
<li><p class="first"><strong>handle_invalid</strong> (<em>bool</em>) – Whether to replace 0/1 estimated rewards with randomly-generated numbers (see Note)</p>
</li>
<li><p class="first"><strong>c</strong> (<em>None or float</em>) – Constant by which to multiply all scores from the exploration policy.</p>
</li>
<li><p class="first"><strong>pmin</strong> (<em>None or float</em>) – Scores (from the exploration policy) will be converted to the minimum between
pmin and the original estimate.</p>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<p>[1] Doubly robust policy evaluation and learning (2011)</p>
</dd></dl>

<dl class="function">
<dt id="contextualbandits.evaluation.evaluateRejectionSampling">
<code class="descclassname">contextualbandits.evaluation.</code><code class="descname">evaluateRejectionSampling</code><span class="sig-paren">(</span><em>policy</em>, <em>X</em>, <em>a</em>, <em>r</em>, <em>online=False</em>, <em>start_point_online='random'</em>, <em>batch_size=10</em><span class="sig-paren">)</span><a class="headerlink" href="#contextualbandits.evaluation.evaluateRejectionSampling" title="Permalink to this definition">¶</a></dt>
<dd><p>Evaluate a policy using rejection sampling on test data.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">In order for this method to be unbiased, the actions on the test sample must have been
collected at random and not according to some other policy.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>policy</strong> (<em>obj</em>) – Policy to be evaluated (already fitted to data). Must have a ‘predict’ method.
If it is an online policy, it must also have a ‘fit’ method.</li>
<li><strong>X</strong> (<em>array (n_samples, n_features)</em>) – Matrix of covariates for the available data.</li>
<li><strong>a</strong> (<em>array (n_samples), int type</em>) – Arms or actions that were chosen for each observation.</li>
<li><strong>r</strong> (<em>array (n_samples), {0,1}</em>) – Rewards that were observed for the chosen actions. Must be binary rewards 0/1.</li>
<li><strong>online</strong> (<em>bool</em>) – Whether this is an online policy to be evaluated by refitting it to the data
as it makes choices on it.</li>
<li><strong>start_point_online</strong> (<em>either str ‘random’ or int in [0, n_samples-1]</em>) – Point at which to start evaluating cases in the sample.
Only used when passing online=True.</li>
<li><strong>batch_size</strong> (<em>int</em>) – After how many rounds to refit the policy being evaluated.
Only used when passing online=True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><strong>result</strong> – Estimated mean reward and number of observations taken.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tuple (float, int)</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<p>[1] A contextual-bandit approach to personalized news article recommendation (2010)</p>
</dd></dl>

</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></li>
<li><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></li>
<li><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></li>
</ul>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, David Cortes.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>