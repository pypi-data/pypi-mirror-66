{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = !git rev-parse --show-toplevel\n",
    "import os; os.chdir(root_dir[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'http://www.atlasbrasil.org.br'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = ''' \n",
    "    -H 'Connection: keep-alive'\n",
    "    -H 'Cache-Control: max-age=0'\n",
    "    -H 'Upgrade-Insecure-Requests: 1'\n",
    "    -H 'User-Agent: Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36'\n",
    "    -H 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3'\n",
    "    -H 'Referer: http://www.atlasbrasil.org.br/2013/pt/consulta/'\n",
    "    -H 'Accept-Language: en-US,en;q=0.9,pt-BR;q=0.8,pt;q=0.7'\n",
    "'''.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl '{BASE_URL}/2013/pt/download/' {HEADERS} --insecure > /tmp/a.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs4.BeautifulSoup(open('/tmp/a.html').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = soup.find('head').find('base').attrs['href']\n",
    "BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloads = soup.find_all(class_='blue_button')\n",
    "links = [b.parent.attrs['href'] for b in downloads]\n",
    "assert  links[0] == 'data/rawData/Indicadores Atlas - RADAR IDHM.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_already_have = [\n",
    "    \"Indicadores Atlas - RADAR IDHM.xlsx\",\n",
    "    \"DADOS_DO_ATLAS_DESAGREGADOS_POR_COR__SEXO_E_DOMICILIO_-_2000_E_2010_-_FINAL.xlsx\",\n",
    "    \"agrupamento_UDHs_para_calculo_no_IBGE_2000_2010.xlsx\",\n",
    "    \"atlas2013_dadosbrutos_pt.xlsx\",\n",
    "    \"dados_belem.zip\",\n",
    "    \"dados_bh.zip\",\n",
    "    \"dados_bs.zip\",\n",
    "    \"dados_campinas.zip\",\n",
    "    \"dados_curitiba.zip\",\n",
    "    \"dados_df.zip\",\n",
    "    \"dados_florianopolis.zip\",\n",
    "    \"dados_fortaleza.zip\",\n",
    "    \"dados_goiania.zip\",\n",
    "    \"dados_maceio.zip\",\n",
    "    \"dados_manaus.zip\",\n",
    "    \"dados_natal.zip\",\n",
    "    \"dados_pa.zip\",\n",
    "    \"dados_petrolina_juazeiro.zip\",\n",
    "    \"dados_recife.zip\",\n",
    "    \"dados_sl.zip\",\n",
    "    \"dados_teresina_timon.zip\",\n",
    "    \"dados_vitoria.zip\"\n",
    "]\n",
    "\n",
    "links_remaining = set(links) - set(links_already_have)\n",
    "links_remaining = list(links_remaining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "for l in links_remaining[:]:\n",
    "    url = BASE_URL + BASE_PATH + urllib.parse.quote(l)\n",
    "    print(url)\n",
    "    file = l.split('/')[-1]\n",
    "    !cd data/raw/; curl \"{url}\" {HEADERS} --insecure -o '{file}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hey, listen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head data/raw/Indicadores\\ Atlas\\ -\\ RADAR\\ IDHM.xlsx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = !ls -1 ../data/raw/*.zip\n",
    "size = !du -s ../data/raw/\n",
    "\n",
    "assert int(size[0].split('\\t')[0]) > 10000, size\n",
    "assert len(lines) == 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unzippin all this crap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rarfile\n",
    "import zipfile\n",
    "from os import listdir, remove, rmdir, rename\n",
    "from os.path import isfile, isdir, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pattern_files(base_path, pattern):\n",
    "    return [ join(base_path,file) for file in listdir(base_path) if isfile(join(base_path, file)) and pattern in file ]\n",
    "\n",
    "def get_all_generated_folders(base_path):\n",
    "    return [ join(base_path,folder) for folder in listdir(data_path) if isdir(join(data_path, folder))]\n",
    "\n",
    "def extract_all(path_to_files, extract_to_path, extension):\n",
    "    extractor = zipfile.ZipFile if extension == '.zip' else rarfile.RarFile\n",
    "    for path_to_file in path_to_files:\n",
    "        with extractor(path_to_file, 'r') as ref:\n",
    "            ref.extractall(extract_to_path)\n",
    "\n",
    "def move_all_files_from_dir_to_target(target_path, dir_path, extension=\".\"):\n",
    "    files_names = [join(dir_path, filename) for filename in listdir(dir_path)]\n",
    "    move_all_files_to_base(target_path, files_names)\n",
    "\n",
    "def move_all_files_to_target(target_path, files_paths):\n",
    "    files_names = [ file_path.split('/')[-1] for file_path in files_paths]\n",
    "    for filename, original_path in zip(files_names, files_paths):\n",
    "        rename(original_path, join(target_path, filename))\n",
    "\n",
    "def flat_list(l):\n",
    "    return [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf ../data/preprocessed\n",
    "mkdir -p ../data/preprocessed\n",
    "mkdir -p ../data/preprocessed/others\n",
    "cp ../data/raw/* ../data/preprocessed/\n",
    "shopt -s extglob\n",
    "cd ../data/preprocessed/ && mv !(dados_*.zip|others) others "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting everything and removing unnecessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/preprocessed'\n",
    "\n",
    "zip_files = get_pattern_files(data_path, '.zip')\n",
    "extract_all(zip_files, data_path, '.zip')\n",
    "for folder_path in get_all_generated_folders(data_path):\n",
    "    move_all_files_from_dir_to_base(data_path, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rar_files = get_pattern_files(data_path, '.rar')\n",
    "extract_all(rar_files, data_path, '.rar')\n",
    "for folder_path in get_all_generated_folders(data_path):\n",
    "    move_all_files_from_dir_to_base(data_path, folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in zip_files + rar_files:\n",
    "    remove(file)\n",
    "directories_to_remove = get_all_generated_folders(data_path)\n",
    "for folder in directories_to_remove:\n",
    "    rmdir(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving each file type to its specific folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -e\n",
    "cd ../data/preprocessed/\n",
    "mkdir udh; mkdir regional; mkdir regiao_metro;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "udh_files = get_pattern_files(data_path, \"UDH\")\n",
    "move_all_files_to_target(join(data_path, \"udh\"), udh_files)\n",
    "\n",
    "regional_files = get_pattern_files(data_path, \"Regional\") + get_pattern_files(data_path, \"REGIO\")\n",
    "move_all_files_to_target(join(data_path, \"regional\"), regional_files)\n",
    "\n",
    "metro_files = get_pattern_files(data_path, \"RM\")\n",
    "move_all_files_to_target(join(data_path, \"regiao_metro\"), metro_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = !ls -1 ../data/preprocessed/udh/*.shp\n",
    "size = !du -s ../data/raw/\n",
    "\n",
    "assert int(size[0].split('\\t')[0]) > 10000, size\n",
    "assert len(lines) == 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Excels to parquets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "excels = glob.glob('../data/preprocessed/udh/*.xlsx')\n",
    "excels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excels.remove('../data/preprocessed/udh/agrupamento_UDHs_para_calculo_no_IBGE_2000_2010.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing, glob, pandas as pd\n",
    "import pyarrow.lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_parquet(path):\n",
    "    print(f'converting {path}')\n",
    "    dfs = pd.read_excel(path, sheet_name=None)\n",
    "    for sheet_name, df in dfs.items():\n",
    "        try:\n",
    "            if df.empty: continue\n",
    "            df.to_parquet(path.replace('.xlsx', '__' + sheet_name + '.parquet'), index=False)\n",
    "        except (Exception, pyarrow.lib.ArrowTypeError) as e:\n",
    "            print('error in:')\n",
    "            print((path, sheet_name, e))\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "res = map(convert_to_parquet, excels)\n",
    "all(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh ../data/preprocessed/udh/*parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Default Python (olx)",
   "language": "python",
   "name": "olx"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
