Global evaluation metrics. Similar to `concise.metrics` or `keras.metrics` but implemented with numpy and intended to be used on the *whole dataset* after training the model. Many evaluation metrics (AUC, F1, ...) namely can't be expressed as an average over minibatches and are hence not implemented in `keras.metrics` (see [this](https://github.com/fchollet/keras/issues/5794) discussion).

***Note:*** All the metrics mask values -1 for classification and `np.nan` for regression.

## Available evaluation metrics

{{autogenerated}}
